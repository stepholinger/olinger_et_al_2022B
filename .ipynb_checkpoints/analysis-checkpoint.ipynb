{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce85f6-7eaf-452c-a969-16bfa6cf6b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import datetime\n",
    "import itertools\n",
    "import numpy as np\n",
    "import types\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import rasterio\n",
    "import obspy\n",
    "from obspy.signal.cross_correlation import correlate, xcorr_max\n",
    "from pyproj import Proj,transform,Geod\n",
    "from scipy.signal import find_peaks,hilbert,butter,filtfilt,correlate,correlation_lags,welch\n",
    "from scipy.integrate import odeint\n",
    "from scipy.ndimage import label, binary_fill_holes\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import dilation, erosion, skeletonize, binary_closing\n",
    "from skimage.filters import gaussian\n",
    "import skan\n",
    "from image_analysis.fracture_mapping import clip, align_images, binary_threshold, read_tsx, plot_fracture_imagery\n",
    "from seismic_analysis.preprocessing import download_data,remove_ir,taper_and_filter\n",
    "from seismic_analysis.visualization import (plot_spectrogram,compute_psd,plot_spectra_and_timeseries,\n",
    "                                plot_tilt_psd_ratio,plot_ringdown,particle_motion)\n",
    "from seismic_analysis.attenuation import estimate_Q,estimate_ringdown\n",
    "from seismic_analysis.location import (compute_backazimuths,get_station_lon_lat,get_crs_locations,travel_time,\n",
    "                                get_station_grid_locations,load_data,get_grid,get_arrivals,get_velocities,\n",
    "                                gridsearch,transform_imagery,plot_imagery_seismic_location)\n",
    "from seismic_analysis.directivity import get_velocity, get_characteristic_frequency\n",
    "from modeling.viscoelastic_beam import get_coefficients,asymptotic_solution,decay_comparison_plot, resonance\n",
    "from modeling.elastic_beam import fg_dispersion_coeffs\n",
    "from modeling.fluid_fracture import find_min_rxx,model_fracture, model_fracture_no_coupling, model_fracture_stopping, plot_fractures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca81aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Transform all TerraSAR-X data to epsg:3245\n",
    "\n",
    "'''\n",
    "\n",
    "files = glob.glob('data/TSX/*/TSX-1.SAR.L1B/*/IMAGEDATA/*')\n",
    "files = [f for f in files if \"epsg:3245\" not in f] \n",
    "for file in files:\n",
    "    transform_imagery(file,'epsg:3245')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e976f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Display TerraSAR-X data from May 2012 to identify new rift and estimate increase in length\n",
    "\n",
    "'''\n",
    "\n",
    "# set TerraSAR-X data file path\n",
    "tsx_scenes = ['dims_op_oc_dfd2_689344819_4','dims_op_oc_dfd2_689344819_5']\n",
    "\n",
    "# read tsx scene before and after fracture\n",
    "before_scene = read_tsx(tsx_scenes[0])\n",
    "after_scene = read_tsx(tsx_scenes[1])\n",
    "\n",
    "# set manually-determined initial rift tip position (epsg:3245 coordinates)\n",
    "initial_rift_tip_pos = [-64800,1736000]\n",
    "\n",
    "# clip a small region of rift and use to compute offset between images\n",
    "cc_bounds = [-65500, -65000, 1719500, 1720500]\n",
    "before_clip = clip(before_scene,cc_bounds)[0]\n",
    "after_clip = clip(after_scene,cc_bounds)[0]\n",
    "x_shift,y_shift = align_images(before_clip,after_clip)\n",
    "\n",
    "# clip to include only rift and align using computed shifts\n",
    "bounds = [-69700, -60150, 1722900, 1743200]\n",
    "before_clip = clip(before_scene,bounds)\n",
    "after_clip = clip(after_scene,bounds)\n",
    "before_shifted = before_clip[0,y_shift:,x_shift:]\n",
    "after_shifted = after_clip[0,:before_shifted.shape[0],:before_shifted.shape[1]]\n",
    "\n",
    "# downsample to speed up image processing\n",
    "before_small = resize(before_shifted, np.asarray(before_shifted.shape)//10)\n",
    "after_small = resize(after_shifted, np.asarray(after_shifted.shape)//10)  \n",
    "\n",
    "# extract binary image of bright areas in before and after images\n",
    "binary_before = binary_threshold(-1*before_small,0.5)\n",
    "binary_after = binary_threshold(-1*after_small,0.6)\n",
    "\n",
    "# make a dilated versions of the before and after images and difference them\n",
    "binary_before = dilation(binary_before,footprint=np.ones((5,5)))\n",
    "binary_after = dilation(binary_after,footprint=np.ones((5,5)))\n",
    "diff = binary_after-binary_before\n",
    "diff[diff < 1] = 0\n",
    "\n",
    "# extract just the rift mask, which is the largest non-background segment\n",
    "labels = label(diff)\n",
    "label_numbers = [np.sum(labels[0] == num) for num in range(labels[1])]\n",
    "label_numbers[0] = np.nan\n",
    "rift_label = np.nanargmax(label_numbers)\n",
    "binary_rift = [labels[0] == rift_label][0]\n",
    "\n",
    "# upsample final new crack area mask to original resolution\n",
    "binary_rift = resize(binary_rift, np.asarray(before_shifted.shape))\n",
    "rift_area_mask = np.ma.masked_where(binary_rift == 0, binary_rift)\n",
    "\n",
    "# extract skeleton representing new length from manually-identified initial point\n",
    "mask_x = np.linspace(bounds[0],bounds[1],rift_area_mask.shape[1])\n",
    "mask_y = np.linspace(bounds[2],bounds[3],rift_area_mask.shape[0])\n",
    "rift_length_mask = binary_rift.copy()\n",
    "rift_length_mask[(np.flipud(mask_y) > initial_rift_tip_pos[1]),:] = 0\n",
    "rift_skeleton = skeletonize(rift_length_mask)\n",
    "skeleton_object = skan.csr.Skeleton(rift_skeleton)\n",
    "\n",
    "# get length of main branch of skeleton\n",
    "branch_summary = skan.csr.summarize(skeleton_object,find_main_branch=True)\n",
    "main_branches = branch_summary[branch_summary.main == True]\n",
    "main_branch_length = np.sum(main_branches['branch-distance'])\n",
    "pixel_size = after_scene.transform[0]\n",
    "rift_length = pixel_size * main_branch_length\n",
    "print(\"R2012 extended by \" + str(np.round(rift_length,2)) +\" meters.\")\n",
    "input_dictionary = {\"length\" : rift_length}\n",
    " \n",
    "# save result\n",
    "file = open('outputs/rift_length.pickle', 'wb')\n",
    "pickle.dump({\"length\" : rift_length}, file)\n",
    "file.close()\n",
    "\n",
    "# plot the result on imagery\n",
    "# background_scenes  = ['dims_op_oc_dfd2_689344819_1','tsx']\n",
    "# tsx_scenes = ['dims_op_oc_dfd2_689344819_4','dims_op_oc_dfd2_689344819_5']\n",
    "# vlims = np.array([[200,450], [200,450]])\n",
    "# plot_bounds = [-80000, -50000, 1715000, 1750000]\n",
    "# features = [[rift_area_mask,bounds,1,'raster','Wistia'],\n",
    "#             [initial_rift_tip_pos,np.nan,0,'point','gold'],\n",
    "#             [initial_rift_tip_pos,np.nan,1,'point','gold']]\n",
    "# plot_fracture_imagery(background_scenes,tsx_scenes,features,plot_bounds,vlims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034f860",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Measure initial length width of R2012\n",
    "\n",
    "'''\n",
    "\n",
    "# set TerraSAR-X data file path\n",
    "tsx_scene = 'dims_op_oc_dfd2_689344819_4'\n",
    "\n",
    "# read tsx scene before and after fracture\n",
    "scene = read_tsx(tsx_scene)\n",
    "\n",
    "# clip to include only widest part of rift\n",
    "bounds = [-65500, -64900, 1741000, 1742000]\n",
    "bounds = [-65500, -64500, 1738000, 1742000]\n",
    "scene_clip = clip(scene,bounds)[0]\n",
    "\n",
    "# extract binary image of bright areas in before and after images\n",
    "binary = binary_threshold(-1*scene_clip,0.88)\n",
    "\n",
    "# extract just the rift mask, which is the largest non-background segment\n",
    "labels = label(binary)\n",
    "label_numbers = [np.sum(labels[0] == num) for num in range(labels[1])]\n",
    "label_numbers[0] = np.nan\n",
    "rift_label = np.nanargmax(label_numbers)\n",
    "binary_rift = [labels[0] == rift_label][0]\n",
    "binary_rift = binary_fill_holes(binary_rift)\n",
    "\n",
    "# clean up edges of binary rift to improve skeleton creation\n",
    "binary_rift_smoothed = binary_closing(binary_rift,footprint=np.ones((50,50)))\n",
    "binary_rift_smoothed = gaussian(binary_rift_smoothed,sigma=2)\n",
    "rift_skeleton = skeletonize(binary_rift_smoothed)\n",
    "skeleton_object = skan.csr.Skeleton(rift_skeleton)\n",
    "\n",
    "# get length of main branch of skeleton\n",
    "branch_summary = skan.csr.summarize(skeleton_object,find_main_branch=True)\n",
    "main_branches = branch_summary[branch_summary.main == True]\n",
    "main_branch_length = np.sum(main_branches['branch-distance'])\n",
    "pixel_size = scene.transform[0]\n",
    "rift_length = pixel_size * main_branch_length\n",
    "\n",
    "# estimate average crack width\n",
    "pixel_area = np.sum(binary_rift)\n",
    "rift_pixel_width = pixel_area / main_branch_length\n",
    "rift_width = pixel_size * rift_pixel_width\n",
    "\n",
    "# report length and width\n",
    "print(\"R2012 initial length: \" + str(np.round(rift_length,2)) +\" meters.\")\n",
    "print(\"R2012 initial mean width: \" + str(np.round(rift_width,2)) +\" meters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be780ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Measure length of R2011, the existing rift that spans most of PIG ice shelf\n",
    "\n",
    "'''\n",
    "\n",
    "# set TerraSAR-X data file path\n",
    "tsx_scene = 'dims_op_oc_dfd2_689344819_4'\n",
    "\n",
    "# read tsx scene before and after fracture\n",
    "scene = read_tsx(tsx_scene)\n",
    "\n",
    "# clip to include only rift\n",
    "bounds = [-69900, -61000, 1714500, 1744500]\n",
    "scene_clip = clip(scene,bounds)[0]\n",
    "\n",
    "# downsample to speed up image processing\n",
    "scene_small = resize(scene_clip, np.asarray(scene_clip.shape)//10)\n",
    "\n",
    "# extract binary image of bright areas in before and after images\n",
    "binary = binary_threshold(-1*scene_small,0.498)\n",
    "binary[binary < 1] = 0\n",
    "\n",
    "# extract just the rift mask, which is the largest non-background segment\n",
    "labels = label(binary)\n",
    "label_numbers = [np.sum(labels[0] == num) for num in range(labels[1])]\n",
    "label_numbers[0] = np.nan\n",
    "rift_label = np.nanargmax(label_numbers)\n",
    "binary_rift = [labels[0] == rift_label][0]\n",
    "\n",
    "# clean up edges of binary rift to improve skeleton creation\n",
    "# binary_rift_smoothed = binary_closing(binary_rift,footprint=np.ones((5,5)))\n",
    "# binary_rift_smoothed = gaussian(binary_rift_smoothed,sigma=1)\n",
    "\n",
    "# upsample final new crack area mask to original resolution\n",
    "binary_rift_smoothed = resize(binary_rift, np.asarray(scene_clip.shape))\n",
    "\n",
    "# create skeleton from cleaned-up and resized binary rift\n",
    "rift_skeleton = skeletonize(binary_rift_smoothed)\n",
    "skeleton_object = skan.csr.Skeleton(rift_skeleton)\n",
    "\n",
    "# get length of main branch of skeleton\n",
    "branch_summary = skan.csr.summarize(skeleton_object,find_main_branch=True)\n",
    "main_branches = branch_summary[branch_summary.main == True]\n",
    "main_branch_length = np.sum(main_branches['branch-distance'])\n",
    "pixel_size = scene.transform[0]\n",
    "rift_length = pixel_size * main_branch_length\n",
    "print(\"R2011 length: \" + str(np.round(rift_length,2)) +\" meters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2e18a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Download seismic data and remove instrumental response\n",
    "\n",
    "'''\n",
    "\n",
    "# download data for May 9 event\n",
    "data_path = \"data/\"\n",
    "starttime = obspy.UTCDateTime(2012, 5, 5)\n",
    "endtime = obspy.UTCDateTime(2012, 5, 12)\n",
    "networks = [\"XC\",\"YT\"]\n",
    "stations = [\"PIG*\",\"DNTW\",\"UPTW\",\"THUR\",\"BEAR\"]\n",
    "channels = [\"HH\",\"BH\"]\n",
    "download_data(data_path,starttime,endtime,networks,stations,channels)\n",
    "\n",
    "# download data for Scotia Sea earthquake\n",
    "starttime = obspy.UTCDateTime(2013, 11, 17)\n",
    "endtime = obspy.UTCDateTime(2013, 11, 18)\n",
    "download_data(data_path,starttime,endtime,[\"XC\"],[\"PIG2\"],[\"HH\"])\n",
    "\n",
    "# remove instrumental response from data\n",
    "responses = ['DISP','VEL']\n",
    "HH_freq_lims = [0.00005,0.0001,45,50]\n",
    "BH_freq_lims = [0.00005,0.0001,15,20]\n",
    "for resp in responses:\n",
    "    remove_ir(data_path,\"HH\",HH_freq_lims,resp)\n",
    "    remove_ir(data_path,\"BH\",BH_freq_lims,resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad260e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Get distance from PIG to each regional station\n",
    "\n",
    "'''\n",
    "\n",
    "pig_lon_lat = get_station_lon_lat(\"data/XML/\",[\"XC\"],[\"PIG2\"])\n",
    "pig_x_y = get_crs_locations(pig_lon_lat,\"epsg:3245\")[0]\n",
    "regional_stations = [\"THUR\",\"UPTW\",\"DNTW\"]\n",
    "for station in regional_stations:\n",
    "    lon_lat = get_station_lon_lat(\"data/XML/\",[\"YT\"],[station])\n",
    "    regional_x_y = get_crs_locations(lon_lat,\"epsg:3245\")[0]\n",
    "    dist = np.sqrt(np.sum(np.square(regional_x_y - pig_x_y)))\n",
    "    print(station + \": \" + str(dist/1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37af87-b10f-403e-a545-a1f1250ed267",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Make spectrograms of May 9 event\n",
    "\n",
    "'''\n",
    "\n",
    "# read data\n",
    "station = \"PIG2\"\n",
    "channels = [\"HHZ\",\"HHN\",\"HHE\"]\n",
    "responses = ['VEL','DISP']\n",
    "\n",
    "# set parameters for spectrogram\n",
    "starttime = obspy.UTCDateTime(2012,5,9,17)\n",
    "endtime = obspy.UTCDateTime(2012,5,9,22)\n",
    "low_freq = 0.0005\n",
    "window_length = 50000\n",
    "n_overlap = 20000\n",
    "\n",
    "# make spectrogram of event\n",
    "for resp in responses:\n",
    "    for channel in channels:\n",
    "        st = obspy.read(\"data/MSEED/no_IR/\" + station + \"/\" + channel + \"/\" + \"2012-05-09*\"+resp+\"*\")\n",
    "        plot_spectrogram(st,starttime,endtime,low_freq,window_length,n_overlap,resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4e2186",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Compute resonant frequency for PIG\n",
    "\n",
    "'''\n",
    "\n",
    "# set parameters\n",
    "L = 70000\n",
    "W = 30000\n",
    "H_i = 500\n",
    "H_w = 700\n",
    "eta = 2e12\n",
    "\n",
    "# calculate period of first 3 modes for length and width of PIG\n",
    "mode_1_L = resonance(L,H_i,H_w,eta,0)\n",
    "mode_2_L = resonance(L,H_i,H_w,eta,1)\n",
    "mode_3_L = resonance(L,H_i,H_w,eta,2)\n",
    "mode_1_W = resonance(W,H_i,H_w,eta,0)\n",
    "mode_2_W = resonance(W,H_i,H_w,eta,1)\n",
    "mode_3_W = resonance(W,H_i,H_w,eta,2)\n",
    "\n",
    "# print results of analysis\n",
    "print(\"First mode (length): \" + str(mode_1_L) + \" s\")\n",
    "print(\"Second mode (length): \" + str(mode_2_L) + \" s\")\n",
    "print(\"Third mode (length): \" + str(mode_3_L) + \" s\")\n",
    "print(\"First mode (width): \" + str(mode_1_W) + \" s\")\n",
    "print(\"Second mode (width): \" + str(mode_2_W) + \" s\")\n",
    "print(\"Third mode (width): \" + str(mode_3_W) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8fc9dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Compare observed and expected ringdown time to reach A_max*e^-pi for FG waves\n",
    "\n",
    "'''\n",
    " \n",
    "# read and preprocess data\n",
    "resp = \"VEL\"\n",
    "st = obspy.read(\"data/MSEED/no_IR/PIG2/HHZ/2012-05-09*\"+resp+\"*\")\n",
    "low_cut = 10**(-3.5)\n",
    "st = taper_and_filter(st,0.001,\"highpass\",low_cut)\n",
    "\n",
    "# get event and noise windows\n",
    "event = st.copy().trim(starttime=obspy.UTCDateTime(2012,5,9,18),endtime=obspy.UTCDateTime(2012,5,9,23,59))\n",
    "noise = st.copy().trim(starttime=obspy.UTCDateTime(2012,5,9,16,30),endtime=obspy.UTCDateTime(2012,5,9,17,30))\n",
    "\n",
    "# estimate noise floor before event\n",
    "noise_floor = np.mean(np.abs(noise[0].data))\n",
    "\n",
    "# set target amplitude for observed and predicted decay times\n",
    "max_amp = np.max(np.abs(st[0].data))\n",
    "#decay_factor = noise_floor/max_amp\n",
    "decay_factor = np.exp(-np.pi)\n",
    "\n",
    "# estimate ringdown time\n",
    "t_ring_obs = estimate_ringdown(event,\"cumulative\")\n",
    "print(\"Observed ringdown time: \" + str(np.round(t_ring_obs/3600,3)) + \" hours\")\n",
    "\n",
    "# set some geometric parameters\n",
    "L = 70000\n",
    "W = 40000\n",
    "h_w_ocean = 700\n",
    "h_i = 500\n",
    "rho_i = 910\n",
    "rho_w = 1000\n",
    "h_w_cavity = h_w_ocean - h_i*(rho_i/rho_w)\n",
    "\n",
    "# compute reflection coefficient from Abrahams et al 2022 assuming we're in the shallow water wave limit\n",
    "R1 = (np.sqrt(h_w_ocean) - np.sqrt(h_w_cavity)) / (np.sqrt(h_w_ocean) + np.sqrt(h_w_cavity))\n",
    "R2 = 0.76\n",
    "\n",
    "# for reflection coefficient R, how many reflections do we need for amplitude to decay to A_max*e^-pi? Solve R^n = A_max*e^-np.pi\n",
    "amp_vect = np.logspace(0,-10,100)\n",
    "n_vect = [[np.log(a)/np.log(R) for a in amp_vect] for R in [R1,R2]]\n",
    "\n",
    "# estimate dominant frequency from displacement and velocity seismograms\n",
    "f = get_characteristic_frequency(st[0].data,st[0].stats.sampling_rate,[low_cut,1],\"fft\",\"median\")\n",
    "\n",
    "# use dispersion relation to estimate phase velocities at characteristic frequencies\n",
    "coeffs = fg_dispersion_coeffs(h_i,h_w_ocean,f)\n",
    "roots = np.roots(coeffs)\n",
    "k = np.real(roots[5])\n",
    "c = f / k\n",
    "\n",
    "# estimate ringdown time\n",
    "t_ring_est_vect = np.array(n_vect)*np.array([[L,W]]).T*2/c\n",
    "t_ring_est = np.log(decay_factor)/np.log(R1)*2*L/c\n",
    "print(\"Estimated ringdown time: \" + str(np.round(t_ring_est/3600,3)) + \" hours\")\n",
    "\n",
    "# make simple plot showing ringdown time\n",
    "plot_ringdown(event,t_ring_est,t_ring_est_vect,amp_vect*max_amp*1000,t_ring_obs,decay_factor*max_amp,resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76f106-0efc-401a-8491-9d9d36a6789f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' \n",
    "\n",
    "Compare spectra of May 9 event and Scotia plate teleseismic signal\n",
    "\n",
    "'''\n",
    "\n",
    "# set windows for both events and noise window before each event\n",
    "rift_event_win = [obspy.UTCDateTime(2012, 5, 9, 18), obspy.UTCDateTime(2012, 5, 9, 19)]\n",
    "rift_noise_win = [obspy.UTCDateTime(2012, 5, 9, 15,15), obspy.UTCDateTime(2012, 5, 9, 16,15)]\n",
    "scotia_event_win = [obspy.UTCDateTime(2013, 11, 17, 9), obspy.UTCDateTime(2013, 11, 17, 10)]\n",
    "scotia_noise_win = [obspy.UTCDateTime(2013, 11, 17, 8), obspy.UTCDateTime(2013, 11, 17, 9)]\n",
    "\n",
    "# set data parameters\n",
    "station = \"PIG2\"\n",
    "channel = \"HHZ\"\n",
    "resp = \"VEL\"\n",
    "low_cut = 10**(-3.5)\n",
    "\n",
    "# read data and select correct station and channel\n",
    "st_rift = obspy.read(\"data/MSEED/no_IR/\" + station + \"/\" + channel + \"/\" + \"2012-05-09*\"+resp+\"*\")\n",
    "st_scotia = obspy.read(\"data/MSEED/no_IR/\" + station + \"/\" + channel + \"/\" + \"2013-11-17*\"+resp+\"*\")\n",
    "st_rift.select(station=station,channel=channel)\n",
    "st_scotia.select(station=station,channel=channel)\n",
    "\n",
    "# trim into event windows and noise windows\n",
    "st_rift_event = st_rift.copy().trim(starttime=rift_event_win[0],endtime=rift_event_win[1])\n",
    "st_rift_noise = st_rift.copy().trim(starttime=rift_noise_win[0],endtime=rift_noise_win[1])\n",
    "st_scotia_event = st_scotia.copy().trim(starttime=scotia_event_win[0],endtime=scotia_event_win[1])\n",
    "st_scotia_noise = st_scotia.copy().trim(starttime=scotia_noise_win[0],endtime=scotia_noise_win[1])\n",
    "\n",
    "# preprocess each window\n",
    "st_rift_event = taper_and_filter(st_rift_event,0.05,\"highpass\",low_cut)\n",
    "st_rift_noise = taper_and_filter(st_rift_noise,0.05,\"highpass\",low_cut)\n",
    "st_scotia_event = taper_and_filter(st_scotia_event,0.05,\"highpass\",low_cut)\n",
    "st_scotia_noise = taper_and_filter(st_scotia_noise,0.05,\"highpass\",low_cut)\n",
    "st_list = [st_rift_event, st_rift_noise, st_scotia_event, st_scotia_noise]\n",
    "\n",
    "# make periodogram of each\n",
    "fs = st_rift_event[0].stats.sampling_rate\n",
    "f, psd_rift_event = compute_psd(st_rift_event[0].data*1000, fs)\n",
    "f, psd_rift_noise = compute_psd(st_rift_noise[0].data*1000, fs)\n",
    "f, psd_scotia_event = compute_psd(st_scotia_event[0].data*1000, fs)\n",
    "f, psd_scotia_noise = compute_psd(st_scotia_noise[0].data*1000, fs)\n",
    "psd_list = [psd_rift_event, psd_rift_noise, psd_scotia_event, psd_scotia_noise]\n",
    "\n",
    "# plot the spectra and timeseries\n",
    "plot_spectra_and_timeseries(st_list,psd_list,f,low_cut,resp)\n",
    "f_rift = get_characteristic_frequency(st_rift_event[0].data,fs,[low_cut,1],\"fft\",\"max\")\n",
    "f_scotia = get_characteristic_frequency(st_scotia_event[0].data,fs,[low_cut,1],\"fft\",\"max\")\n",
    "print(\"Dominant rift event period: \" + str(np.round(1/f_rift,2)) + \" s\")\n",
    "print(\"Dominant Scotia Sea earthquake period: \" + str(np.round(1/f_scotia,2)) + \" s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60508a-64ee-4734-827a-e06953793628",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Determine backazimuth of the event using polarization directions at local stations\n",
    "\n",
    "'''\n",
    "\n",
    "# initialize location parameter object and set parameters for backazimuth computation\n",
    "l = types.SimpleNamespace()\n",
    "l.win_len = 50\n",
    "l.slide = 5\n",
    "l.trace_len = 8*60\n",
    "l.num_steps = int((l.trace_len-l.win_len)/l.slide)+1\n",
    "l.stations = [\"PIG2\",\"PIG4\",\"PIG5\"]\n",
    "l.network = [\"XC\"]\n",
    "\n",
    "# specify whether to use velocity or displacement seismogram\n",
    "l.response = \"VEL\"\n",
    "\n",
    "# specify path to data, XML, and output\n",
    "l.data_path = \"data/MSEED/no_IR\"\n",
    "l.xml_path = \"data/XML\"\n",
    "l.filename = \"outputs/locations/\" + \"_\".join(l.stations) + \"_backazimuth\"\n",
    "l.n_procs = 10\n",
    "\n",
    "# set event start time\n",
    "event_start = datetime.datetime(2012, 5, 9, 18, 1)\n",
    "event_end = event_start + datetime.timedelta(seconds=l.trace_len)\n",
    "l.detection_times = np.array([event_start])\n",
    "\n",
    "# set the coordinate system in which we will do all grid-based calculations\n",
    "l.crs = \"EPSG:3245\"\n",
    "\n",
    "# set signal-to-noise ratio for throwing out stations and sta/lta ratio for throwing out individual windows in backazimuth computation\n",
    "l.snr_threshold = 0\n",
    "l.stalta_threshold = 0\n",
    "\n",
    "# specify method for correcting pca components \n",
    "l.pca_correction = \"distance\"\n",
    "l.centroid = \"fixed\"\n",
    "\n",
    "# specify parameters for cross correlation based determination of station of first arrival\n",
    "l.max_shift = 1000\n",
    "\n",
    "# set frequency band for backazimuth calculation\n",
    "l.fs = 100\n",
    "l.freq = [1,5]\n",
    "\n",
    "# calculate backazimuth for the event\n",
    "b = compute_backazimuths(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d807f258-57f6-4ce9-bf03-e4b83b536f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Locate the event using travel time inversion of arrivals at regional stations\n",
    "\n",
    "''' \n",
    " \n",
    "# read data\n",
    "resp = \"VEL\"\n",
    "st = obspy.read(\"data/MSEED/no_IR/*/*/2012-05-09*\"+resp+\"*\")\n",
    "\n",
    "# set frequency band for analysis\n",
    "freq = [1.5]\n",
    "\n",
    "# choose component to use for location\n",
    "components = [\"Z\",\"N\",\"E\"]\n",
    "\n",
    "# set starttime and endtime for analysis\n",
    "starttime=obspy.UTCDateTime(2012,5,9,18,1)\n",
    "endtime=obspy.UTCDateTime(2012,5,9,18,9)\n",
    "\n",
    "# set on-ice phase velocity\n",
    "local_velocity = 1500\n",
    "\n",
    "# define grid origin in lat,lon and grid dimensions in meters\n",
    "origin_lat_lon = [-75.5,-103.2]\n",
    "grid_length = 1.5e5\n",
    "grid_height = 1.3e5\n",
    "step = 1000\n",
    "t0 = -10\n",
    "t_step = 0.1\n",
    "\n",
    "# select, filter, and trim regional data\n",
    "ref_station = \"PIG2\"\n",
    "local_stations = [\"PIG2\",\"PIG4\",\"PIG5\"]\n",
    "regional_stations = [\"THUR\",\"DNTW\",\"BEAR\",\"UPTW\"]\n",
    "st_local, st_regional = load_data(st,local_stations,regional_stations,components,freq,starttime,endtime)\n",
    "\n",
    "# estimate phase velocity of waves from local reference station to regional stations\n",
    "velocities = get_velocities(ref_station,st_local,local_stations,st_regional,regional_stations,components,local_velocity)\n",
    "\n",
    "# combine into one stream and remove any stations, if desired\n",
    "stations = local_stations + regional_stations\n",
    "st = st_local + st_regional\n",
    "\n",
    "# get arrival times relative to PIG2\n",
    "arrivals, xcorr_coefs = get_arrivals(ref_station,st,stations,components)\n",
    "#weights = np.abs(xcorr_coefs)\n",
    "\n",
    "# set grid points and compute distance from grid origin to each station\n",
    "x_vect,y_vect,t_vect = get_grid(grid_length,grid_height,step,t0,t_step)\n",
    "station_lat_lon = np.fliplr(get_station_lon_lat(\"data/XML/\",[\"XC\",\"YT\"],stations))\n",
    "station_x_y = get_station_grid_locations(origin_lat_lon,station_lat_lon)\n",
    "\n",
    "# carry out the gridsearch\n",
    "rmse_mat = gridsearch(t_vect,x_vect,y_vect,station_x_y,velocities,arrivals)\n",
    "\n",
    "# find lowest error lat, lon, and origin time\n",
    "loc_idx = np.unravel_index(np.argmin(rmse_mat), rmse_mat.shape)\n",
    "rmse_loc = rmse_mat[loc_idx[0],:,:]\n",
    "\n",
    "# save error matrix\n",
    "rmse_file = open(\"outputs/locations/\"+\"_\".join(stations)+\"_ref:\"+ref_station+\"_\"+resp+\"_gridsearch.pickle\", \"wb\")\n",
    "pickle.dump(rmse_mat, rmse_file)\n",
    "rmse_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a676d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Plot the backazimuthal and gridsearch locations on imagery\n",
    "\n",
    "'''\n",
    "\n",
    "# load background images\n",
    "background_scenes  = ['dims_op_oc_dfd2_689344819_1','tsx']\n",
    "\n",
    "# load foreground TerraSAR-X data- first scene will be 'before' picture and second will be 'after'\n",
    "tsx_scenes = ['dims_op_oc_dfd2_690829826_1','dims_op_oc_dfd2_689344819_5']\n",
    "\n",
    "# set brightness for each TSX scene\n",
    "vlims = np.array([[80,300], [200,450]])\n",
    "\n",
    "# set bounds for plot\n",
    "bounds = [-97500, -30045, 1698290, 1761950]\n",
    "\n",
    "# read seismic data\n",
    "resp = \"VEL\"\n",
    "st = obspy.read(\"data/MSEED/no_IR/*/*/2012-05-**\"+resp+\"*\")\n",
    "st = st.select(component=\"Z\",station=\"PIG2\")\n",
    "st_low = st.copy().filter('bandpass',freqmin=0.001,freqmax=1)\n",
    "st_high = st.copy().filter('highpass',freq=1)\n",
    "st_low = st_low.merge()\n",
    "st_high = st_high.merge()\n",
    "\n",
    "# load the results of polarization analysis\n",
    "baz_file = open('outputs/locations/PIG2_PIG4_PIG5_backazimuth.pickle', \"rb\")\n",
    "b = pickle.load(baz_file)\n",
    "backazimuth = b.backazimuths\n",
    "baz_file.close()\n",
    "\n",
    "# load the results of gridsearch location\n",
    "ref_stat = \"PIG2\"\n",
    "grid_file = open('outputs/locations/PIG2_PIG4_PIG5_THUR_DNTW_BEAR_UPTW_ref:'+ref_stat+\"_\"+resp+'_gridsearch.pickle', \"rb\")\n",
    "rmse_mat = pickle.load(grid_file)\n",
    "grid_file.close()\n",
    "loc_idx = np.unravel_index(np.argmin(rmse_mat), rmse_mat.shape)\n",
    "rmse_loc = rmse_mat[loc_idx[0],:,:]\n",
    "\n",
    "# get local station locations and array centroids\n",
    "station_lon_lat_coords = get_station_lon_lat(\"data/XML/\",[\"XC\"],[\"PIG2\",\"PIG4\",\"PIG5\"])\n",
    "station_grid_coords = get_crs_locations(station_lon_lat_coords,\"epsg:3245\")\n",
    "\n",
    "# get gridsearch axes\n",
    "origin_lat = -75.5\n",
    "origin_lon = -103.2\n",
    "grid_length = 1.5e5\n",
    "grid_height = 1.3e5\n",
    "step = 1000\n",
    "x_vect = np.arange(0, grid_length, step)\n",
    "y_vect = np.arange(0, grid_height, step)\n",
    "p2 = Proj(\"EPSG:3245\",preserve_units=False)\n",
    "p1 = Proj(proj='latlong',preserve_units=False)\n",
    "origin_x,origin_y = transform(p1,p2,origin_lon,origin_lat)\n",
    "grid_axes_coords = [origin_x + x_vect, origin_y + y_vect]\n",
    "\n",
    "# make plot of location\n",
    "plot_imagery_seismic_location(background_scenes,tsx_scenes,bounds,st_low,st_high,backazimuth,rmse_loc,station_grid_coords,grid_axes_coords,vlims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48aae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Quantify horizontal-to-vertical amplitude ratio\n",
    "\n",
    "'''\n",
    "\n",
    "# read seismic data\n",
    "resp = \"DISP\"\n",
    "st = obspy.read(\"data/MSEED/no_IR/*/*/2012-05-09*\"+resp+\"*\")\n",
    "st = st.select(station=\"PIG2\")\n",
    "st.trim(starttime=obspy.UTCDateTime(2012, 5, 9, 18), endtime=obspy.UTCDateTime(2012, 5, 9, 20))\n",
    "\n",
    "# load the results of polarization analysis\n",
    "baz_file = open('outputs/locations/PIG2_PIG4_PIG5_backazimuth.pickle', \"rb\")\n",
    "b = pickle.load(baz_file)\n",
    "backazimuth = b.backazimuths\n",
    "baz_file.close()\n",
    "\n",
    "# rotate based on computed backazimuth\n",
    "st.rotate('NE->RT',back_azimuth=backazimuth)\n",
    "\n",
    "# filter, taper and downsample the data\n",
    "band = [0.001,1]\n",
    "st = taper_and_filter(st,0.05,\"bandpass\",band)\n",
    "st = st.resample(band[1]*3)\n",
    "\n",
    "# compute fft of vertical and horizontal seismogram\n",
    "z_data = st.select(component=\"Z\")[0].data\n",
    "z_spectra = np.fft.fft(z_data)\n",
    "r_data = st.select(component=\"R\")[0].data\n",
    "r_spectra = np.fft.fft(r_data)\n",
    "f = np.fft.fftfreq(len(z_data),st[0].stats.sampling_rate)\n",
    "f_pos = f[1:len(f)//2]\n",
    "\n",
    "# use the FG wave dispersion relation to compute wavenumber vector\n",
    "h_i = 550\n",
    "h_w = 700\n",
    "roots = []\n",
    "for freq in f:\n",
    "    coeffs = fg_dispersion_coeffs(h_i,h_w,freq)\n",
    "    roots.append(np.roots(coeffs))\n",
    "roots = np.array(roots)\n",
    "k = roots[:,5]\n",
    "\n",
    "# compute tilt contribution from vertical displacements in the frequency domain\n",
    "tilt_spectra = z_spectra*(-1j*9.8*2*np.pi*k)/((2*np.pi*f)**2)\n",
    "\n",
    "# compute psd and divide to get psd ratio at each frequency\n",
    "r_psd = (np.abs(r_spectra)**2)/(f[1]-f[0])\n",
    "tilt_psd = (np.abs(tilt_spectra)**2)/(f[1]-f[0])\n",
    "r_pos = r_psd[1:len(f)//2]\n",
    "psd_ratio = r_pos/tilt_psd[1:len(f)//2]\n",
    "\n",
    "# make a plot of the ratio of psd\n",
    "plot_tilt_psd_ratio(f_pos,psd_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81bd260",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Measure e-folding time and Q value from observations\n",
    "\n",
    "'''\n",
    "\n",
    "# set station and channel\n",
    "station = \"PIG2\"\n",
    "channel = \"HHZ\"\n",
    "response = \"VEL\"\n",
    "\n",
    "# read and process seismic data\n",
    "st = obspy.read(\"data/MSEED/no_IR/\" + station + \"/\" + channel + \"/2012-05-09.\" + station + \".\" + channel + \".\" + response + \".no_IR\" + \".MSEED\")\n",
    "\n",
    "# set frequency bands (below values are log space)\n",
    "nyquist = np.log10(st[0].stats.sampling_rate/2)\n",
    "freq_low = -3\n",
    "num_steps = 35\n",
    "step_size = (nyquist-freq_low)/num_steps\n",
    "bands = [-3 + step_size*i for i in range(num_steps)]\n",
    "if np.sum([bands == 0]) > 0: bands.remove(0)\n",
    "\n",
    "# estimate Q in each frequency band\n",
    "Q_vect = []\n",
    "T_vect = []\n",
    "for i in range(len(bands)-1):\n",
    "    # estimate Q for current frequency band\n",
    "    band = [10**bands[i],10**bands[i+1]]\n",
    "    starttime = obspy.UTCDateTime(2012, 5, 9, 18)\n",
    "    endtime = obspy.UTCDateTime(2012, 5, 9, 19,30)\n",
    "    T,Q = estimate_Q(st.copy(),band,[starttime,endtime])\n",
    "    Q_vect = np.append(Q_vect,Q)\n",
    "    T_vect = np.append(T_vect,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d167af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Model viscoelastic beam to see whether viscous damping can explain observed ringdown time \n",
    "\n",
    "'''\n",
    "\n",
    "# set ice shelf geometry (length is used to set lowest wavenumber)\n",
    "H_i = 550\n",
    "H_w = 700\n",
    "L = 7e4\n",
    "\n",
    "# define a range of wavenumbers\n",
    "xi_vect = np.logspace(-5,np.log10(1000/H_i),100)\n",
    "\n",
    "# define material properties\n",
    "eta = 2e12\n",
    "E = 8.7e9\n",
    "nu = 0.3\n",
    "mu = E/2/(1+nu)\n",
    "t_m = eta/mu\n",
    "\n",
    "# choose which physics to include\n",
    "flexure = 1\n",
    "water_waves = 1\n",
    "inertia = 1\n",
    "buoyancy = 1\n",
    "physics = [flexure,water_waves,inertia,buoyancy]\n",
    "\n",
    "# iterate through wavenumber vector and find roots\n",
    "roots = []\n",
    "for xi in xi_vect:\n",
    "    coefficients = get_coefficients(H_i,H_w,xi,eta,physics)\n",
    "    roots.append(np.roots(coefficients))\n",
    "roots = np.array(roots)\n",
    "\n",
    "# get Taylor series approximations of the analytical expression for Q \n",
    "approximate_solutions = []\n",
    "for xi in xi_vect:\n",
    "    approximate_solutions.append(asymptotic_solution(xi,H_i,H_w,eta))\n",
    "approximate_solutions = np.array(approximate_solutions)\n",
    "\n",
    "# make plot comparing numerical Q and asymptotic approximation to Q\n",
    "period_of_interest = 500\n",
    "decay_comparison_plot(xi_vect,roots,approximate_solutions[:,3],T_vect,Q_vect,eta,E,period_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Plot particle motion at each station\n",
    "\n",
    "'''\n",
    "\n",
    "# read data\n",
    "resp = \"DISP\"\n",
    "st = obspy.read(\"data/MSEED/no_IR/*/*/2012-05-09*\"+resp+\"*\")\n",
    "\n",
    "# set frequency band for analysis\n",
    "freq = [1,5]\n",
    "fs = 2*freq[1]\n",
    "\n",
    "# set starttime and endtime for analysis\n",
    "starttime=obspy.UTCDateTime(2012,5,9,18,2)\n",
    "endtime=obspy.UTCDateTime(2012,5,9,18,10)\n",
    "\n",
    "# select, filter, and trim local data\n",
    "stations = [\"THUR\",\"DNTW\",\"UPTW\"]\n",
    "st = taper_and_filter(st,0.05,\"bandpass\",freq)\n",
    "st.trim(starttime=starttime,endtime=endtime)\n",
    "st.resample(fs)\n",
    "\n",
    "for station in stations:\n",
    "    # load the results of polarization analysis\n",
    "    baz_file = open('outputs/locations/'+station+'_backazimuth.pickle', \"rb\")\n",
    "    b = pickle.load(baz_file)\n",
    "    backazimuth = b.backazimuths\n",
    "    baz_file.close()\n",
    "\n",
    "    # rotate seismogram\n",
    "    st_station = st.copy().select(station=station)\n",
    "    st_station.rotate('NE->RT',backazimuth)\n",
    "\n",
    "    # make particle motion plot\n",
    "    particle_motion(st_station,20,[\"R\",\"Z\"])\n",
    "    particle_motion(st_station,20,[\"R\",\"T\"])\n",
    "    particle_motion(st_station,20,[\"T\",\"Z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28921f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Estimate average rupture velocity using known crack length and duration of high-frequency emissions\n",
    "\n",
    "'''\n",
    " \n",
    "# read and preprocess data\n",
    "resp = \"VEL\"\n",
    "st = obspy.read(\"data/MSEED/no_IR/PIG2/HHZ/2012-05-09*\"+resp+\"*\")\n",
    "freq = 0.5\n",
    "st = taper_and_filter(st,0.001,\"highpass\",freq)\n",
    "\n",
    "# get event and noise windows\n",
    "event = st.copy().trim(starttime=obspy.UTCDateTime(2012,5,9,18,1,30),endtime=obspy.UTCDateTime(2012,5,9,18,10))\n",
    "\n",
    "# estimate duration (95% of cumulative amplitude)\n",
    "duration = estimate_ringdown(event,\"cumulative\")\n",
    "\n",
    "# read in fracture length\n",
    "with open('outputs/rift_length.pickle', 'rb') as f:\n",
    "    length = pickle.load(f)['length']\n",
    "\n",
    "# estimate crack tip speed\n",
    "v_s = length/duration\n",
    "print(\"Observed length: \" + str(np.round(length,2)) + \" m\")\n",
    "print(\"Observed duration: \" + str(np.round(duration,2)) + \" s\")\n",
    "print(\"Predicted source velocity: \" + str(np.round(v_s,2)) + \" m/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653bae4-aa4f-45b7-9e0a-998247622c50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Model coupled fluid-fracture system to test whether fluid processes can explain slow rift tip velocity\n",
    "\n",
    "ADD REGRESSION WITH AVERAGE SPEED\n",
    "\n",
    "ADD PROCESS NUMBERS TO PLOTS\n",
    "\n",
    "'''\n",
    "\n",
    "H_i = 200\n",
    "rho_i = 910\n",
    "rho_w = 1000\n",
    "H_w = rho_i/rho_w * H_i\n",
    "g = 9.8\n",
    "c_r = 2000\n",
    "eta0 = H_w\n",
    "d_eta_dt0 = 0\n",
    "L0 = 3890\n",
    "w0 = 91\n",
    "K_ic = 1e5\n",
    "sigma0 = -rho_i*g*H_i/2 + rho_w*g/(2*H_i)*eta0**2\n",
    "epsilon = 40\n",
    "\n",
    "# The following calculates Rxx such that K_ic = K_i, and then adds a tiny perturbation\n",
    "Rxx = find_min_rxx(L0,sigma0,K_ic)[0]+4\n",
    "\n",
    "# run model with fluid coupling for long timescale\n",
    "t = np.linspace(0, 2000, 1000)\n",
    "y0 = [eta0, d_eta_dt0,L0]\n",
    "sol = odeint(model_fracture, y0, t, args=(epsilon, H_i, c_r, Rxx, w0))\n",
    "\n",
    "# run model without fluid coupling\n",
    "y0 = [L0]\n",
    "sol_no_coupling = odeint(model_fracture_no_coupling, y0, t, args=(epsilon, H_i, Rxx, w0))\n",
    "\n",
    "# get some useful variables\n",
    "dLdt = np.gradient(sol[:, 2],t)\n",
    "dLdt_no_coupling = np.gradient(sol_no_coupling[:, 0],t)\n",
    "\n",
    "# set length of interest\n",
    "L1 = L0 + 10500\n",
    "\n",
    "# subset before length\n",
    "L = sol[:, 2]\n",
    "idx = np.where(L < L1)[0]\n",
    "dLdt = np.gradient(sol[:,2][idx],t[idx])\n",
    "c_avg = np.round(np.mean(dLdt),2)\n",
    "dLdt_no_coupling = np.gradient(sol_no_coupling[:,0][idx],t[idx])\n",
    "\n",
    "# plot fracture for entire runtime\n",
    "plot_fractures(t,sol,sol_no_coupling,c_r,H_w,L0,L1,c_avg,[[-0.15,2.5],[0,1.1],[L0/1000,np.max(sol[:,2])/1000],[L0/1000,L1/1000]])\n",
    "\n",
    "# report average propagation rate\n",
    "print(\"Mean crack propagation rate (epsilon=\"+str(epsilon)+\"):\",c_avg,\" m/s\")\n",
    "print(\"Mean crack propagation rate with no coupling (epsilon=\"+str(epsilon)+\"):\",np.round(np.mean(dLdt_no_coupling),2),\" m/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d0a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sermeq_env",
   "language": "python",
   "name": "sermeq_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
